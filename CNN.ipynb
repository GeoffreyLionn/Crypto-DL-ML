{"metadata":{"colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9055670,"sourceType":"datasetVersion","datasetId":5460295}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Import Libraries","metadata":{"id":"33da6550"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nimport os, time, datetime\nfrom collections import OrderedDict\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_score, accuracy_score\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay","metadata":{"executionInfo":{"elapsed":357,"status":"ok","timestamp":1719851799360,"user":{"displayName":"Geoffrey Lion","userId":"10007001253387755890"},"user_tz":-60},"id":"75346719","execution":{"iopub.status.busy":"2024-08-14T12:22:58.276989Z","iopub.execute_input":"2024-08-14T12:22:58.277340Z","iopub.status.idle":"2024-08-14T12:22:59.159097Z","shell.execute_reply.started":"2024-08-14T12:22:58.277308Z","shell.execute_reply":"2024-08-14T12:22:59.158193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter","metadata":{"execution":{"iopub.status.busy":"2024-08-14T12:22:59.164279Z","iopub.execute_input":"2024-08-14T12:22:59.165146Z","iopub.status.idle":"2024-08-14T12:23:04.832649Z","shell.execute_reply.started":"2024-08-14T12:22:59.165109Z","shell.execute_reply":"2024-08-14T12:23:04.831500Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data","metadata":{"id":"4634af74"}},{"cell_type":"code","source":"data = pd.read_csv('../input/BTC_OHLC.csv')\n\n# Moving Average\ndata['MA5'] = data['Close'].rolling(window = 5).mean()\ndata['MA20'] = data['Close'].rolling(window = 20).mean()\n\n# Label\ndata.loc[:, 'Return'] = data['Close'].shift(-1) / data['Close'] - 1\n\ndef label(x):\n    if x > 0:\n        return 1\n    else:\n        return 0\n    \ndata['label'] = data['Return'].apply(lambda x: label(x))\n\n# Drop NA Rows\ndata.dropna(inplace = True)","metadata":{"executionInfo":{"elapsed":76872,"status":"ok","timestamp":1719851890980,"user":{"displayName":"Geoffrey Lion","userId":"10007001253387755890"},"user_tz":-60},"id":"d9e97b99","execution":{"iopub.status.busy":"2024-08-14T12:23:04.833754Z","iopub.execute_input":"2024-08-14T12:23:04.834332Z","iopub.status.idle":"2024-08-14T12:23:04.862302Z","shell.execute_reply.started":"2024-08-14T12:23:04.834305Z","shell.execute_reply":"2024-08-14T12:23:04.861397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Charts Construction (Matrix):\n1. image (OHLC)\n2. image_vol (OHLC + Volume)\n3. image_vol_ma (OHLC + Volume + MA)","metadata":{"id":"a26b4e65"}},{"cell_type":"code","source":"def image(data):\n    num_days = len(data)\n    image_height, image_width = 64, 3 * num_days\n    mini, maxi = data['Low'].min(), data['High'].max()\n\n    normalized_high = np.interp(data['High'], (mini, maxi), (0, image_height - 1))\n    normalized_low = np.interp(data['Low'], (mini, maxi), (0, image_height - 1))\n    normalized_open = np.interp(data['Open'], (mini, maxi), (0, image_height - 1))\n    normalized_close = np.interp(data['Close'], (mini, maxi), (0, image_height - 1))\n\n    chart_matrix = np.zeros((image_height, image_width), dtype=np.uint8)\n    bar_width = max(1, image_width // num_days)\n\n    for idx, (o, h, l, c) in enumerate(zip(normalized_open, normalized_high, normalized_low, normalized_close)):\n        x_position = 3 * idx + 1\n        open_pixel = image_height - int(o) - 1\n        high_pixel = image_height - int(h) - 1\n        low_pixel = image_height - int(l) - 1\n        close_pixel = image_height - int(c) - 1\n\n        # Open Tick\n        chart_matrix[open_pixel, x_position - 1] = 255\n\n        # Close Tick\n        chart_matrix[close_pixel, x_position + 1] = 255\n\n        # High-Low Body\n        chart_matrix[high_pixel:low_pixel + 1, x_position] = 255\n\n    return chart_matrix\n\ndef image_vol(data):\n    num_days = len(data)\n    image_height, image_width = 64, 3 * num_days\n\n    mini, maxi = data['Low'].min(), data['High'].max()\n    normalized_high = np.interp(data['High'], (mini, maxi), (12, image_height - 1))\n    normalized_low = np.interp(data['Low'], (mini, maxi), (12, image_height - 1))\n    normalized_open = np.interp(data['Open'], (mini, maxi), (12, image_height - 1))\n    normalized_close = np.interp(data['Close'], (mini, maxi), (12, image_height - 1))\n    vol_mini, vol_maxi = data['Volume'].min(), data['Volume'].max()\n    normalized_vol = np.interp(data['Volume'], (vol_mini, vol_maxi), (0, 11))\n    chart_matrix = np.zeros((image_height, image_width), dtype=np.uint8)\n    bar_width = max(1, image_width // num_days)\n\n    for idx, (o, h, l, c, v) in enumerate(zip(normalized_open, normalized_high, normalized_low, normalized_close, normalized_vol)):\n        x_position = 3 * idx + 1\n        open_pixel = image_height - int(o) - 1\n        high_pixel = image_height - int(h) - 1\n        low_pixel = image_height - int(l) - 1\n        close_pixel = image_height - int(c) - 1\n\n        # Open Tick\n        chart_matrix[open_pixel, x_position-1] = 255\n\n        # Close Tick\n        chart_matrix[close_pixel, x_position + 1] = 255\n\n        # High-Low Body\n        chart_matrix[high_pixel:low_pixel + 1, x_position] = 255\n\n        # Volume\n        chart_matrix[image_height - int(v) - 1:image_height-1, x_position] = 255\n\n    return chart_matrix\n\ndef image_vol_ma(data):\n    num_days = len(data)\n    image_height, image_width = 64, 3 * num_days\n\n    mini, maxi = min(data['Low'].min(), data['MA20'].min()), max(data['MA20'].max(), data['High'].max())\n    normalized_high = np.interp(data['High'], (mini, maxi), (12, image_height - 1))\n    normalized_low = np.interp(data['Low'], (mini, maxi), (12, image_height - 1))\n    normalized_open = np.interp(data['Open'], (mini, maxi), (12, image_height - 1))\n    normalized_close = np.interp(data['Close'], (mini, maxi), (12, image_height - 1))\n    normalized_ma = np.interp(data['MA20'], (mini, maxi), (12, image_height - 1))\n    vol_mini, vol_maxi = data['Volume'].min(), data['Volume'].max()\n    normalized_vol = np.interp(data['Volume'], (vol_mini, vol_maxi), (0, 11))\n\n    chart_matrix = np.zeros((image_height, image_width), dtype=np.uint8)\n    bar_width = max(1, image_width // num_days)\n\n    for idx, (o, h, l, c, v, ma) in enumerate(zip(normalized_open, normalized_high, normalized_low, normalized_close, normalized_vol, normalized_ma)):\n        x_position = 3 * idx + 1\n        open_pixel = image_height - int(o) - 1\n        high_pixel = image_height - int(h) - 1\n        low_pixel = image_height - int(l) - 1\n        close_pixel = image_height - int(c) - 1\n\n        # Open Tick\n        chart_matrix[open_pixel, x_position-1] = 255\n\n        # Close Tick\n        chart_matrix[close_pixel, x_position+1] = 255\n\n        # High-Low Body\n        chart_matrix[high_pixel:low_pixel+1, x_position] = 255\n\n        # Volume\n        chart_matrix[image_height-int(v)-1:image_height-1, x_position] = 255\n\n        # MA\n        chart_matrix[image_height-int(ma)-1, x_position-1:x_position+2] = 255\n\n    return chart_matrix","metadata":{"executionInfo":{"elapsed":388,"status":"ok","timestamp":1719851932229,"user":{"displayName":"Geoffrey Lion","userId":"10007001253387755890"},"user_tz":-60},"id":"1b7f2663","execution":{"iopub.status.busy":"2024-08-14T12:23:04.865720Z","iopub.execute_input":"2024-08-14T12:23:04.866168Z","iopub.status.idle":"2024-08-14T12:23:04.893666Z","shell.execute_reply.started":"2024-08-14T12:23:04.866140Z","shell.execute_reply":"2024-08-14T12:23:04.892324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### train-val-test split","metadata":{"id":"8b550b1e"}},{"cell_type":"code","source":"images, label = [], []\nfor i in range(19, len(data)):\n    images.append(image_vol_ma(data.loc[data.index[i-19:i+1], :]))\n    label.append(data.loc[data.index[i], 'label'])","metadata":{"execution":{"iopub.status.busy":"2024-08-14T12:23:04.894862Z","iopub.execute_input":"2024-08-14T12:23:04.895203Z","iopub.status.idle":"2024-08-14T12:23:09.253813Z","shell.execute_reply.started":"2024-08-14T12:23:04.895174Z","shell.execute_reply":"2024-08-14T12:23:09.252759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_images, train_label = images[:2515], label[:2515]\nX_train, X_val, y_train, y_val = train_test_split(train_images, train_label, test_size=0.2, random_state=1)\nX_test, y_test = images[2515:3611], label[2515:3611]","metadata":{"execution":{"iopub.status.busy":"2024-08-14T12:23:09.255113Z","iopub.execute_input":"2024-08-14T12:23:09.255426Z","iopub.status.idle":"2024-08-14T12:23:09.263644Z","shell.execute_reply.started":"2024-08-14T12:23:09.255401Z","shell.execute_reply":"2024-08-14T12:23:09.262727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Predict the close price of (2021-01-01 to 2023-12-31) for its position at the closing of (2020-12-31 to 2023-12-30)","metadata":{}},{"cell_type":"markdown","source":"### Modelling","metadata":{}},{"cell_type":"code","source":"class CNN20d(nn.Module):\n    # Input: [N, (1), 64, 60]; Output: [N, 2]\n    # Three Convolution Blocks\n    \n    def init_weights(self, m):\n        if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n            torch.nn.init.xavier_uniform_(m.weight)\n            m.bias.data.fill_(0.01)\n    \n    def __init__(self):\n        super(CNN20d, self).__init__()\n        self.conv1 = nn.Sequential(OrderedDict([\n            ('Conv', nn.Conv2d(1, 64, (5, 3), padding=(3, 1), stride=(3, 1), dilation=(2, 1))), # output size: [N, 64, 21, 60]\n            ('BN', nn.BatchNorm2d(64, affine=True)),\n            ('ReLU', nn.ReLU()),\n            ('Max-Pool', nn.MaxPool2d((2,1))) # output size: [N, 64, 10, 60]\n        ]))\n        self.conv1 = self.conv1.apply(self.init_weights)\n        \n        self.conv2 = nn.Sequential(OrderedDict([\n            ('Conv', nn.Conv2d(64, 128, (5, 3), padding=(3, 1), stride=(1, 1), dilation=(1, 1))), # output size: [N, 128, 12, 60]\n            ('BN', nn.BatchNorm2d(128, affine=True)),\n            ('ReLU', nn.ReLU()),\n            ('Max-Pool', nn.MaxPool2d((2,1))) # output size: [N, 128, 6, 60]\n        ]))\n        self.conv2 = self.conv2.apply(self.init_weights)\n        \n        self.conv3 = nn.Sequential(OrderedDict([\n            ('Conv', nn.Conv2d(128, 256, (5, 3), padding=(2, 1), stride=(1, 1), dilation=(1, 1))), # output size: [N, 256, 6, 60]\n            ('BN', nn.BatchNorm2d(256, affine=True)),\n            ('ReLU', nn.ReLU()),\n            ('Max-Pool', nn.MaxPool2d((2,1))) # output size: [N, 256, 3, 60]\n        ]))\n        self.conv3 = self.conv3.apply(self.init_weights)\n\n        self.DropOut = nn.Dropout(p=0.5)\n        self.FC = nn.Linear(46080, 2)\n        self.init_weights(self.FC)\n        self.Softmax = nn.Softmax(dim=1)\n\n    def forward(self, x): # input: [N, 64, 60]\n        x = x.unsqueeze(1).to(torch.float32)   # output size: [N, 1, 64, 60]\n        x = self.conv1(x) # output size: [N, 64, 10, 60]\n        x = self.conv2(x) # output size: [N, 128, 6, 60]\n        x = self.conv3(x) # output size: [N, 256, 3, 60]\n        x = self.DropOut(x.view(x.shape[0], -1))\n        x = self.FC(x) # output size: [N, 2]\n        x = self.Softmax(x)\n        \n        return x","metadata":{"execution":{"iopub.status.busy":"2024-08-14T12:23:09.265089Z","iopub.execute_input":"2024-08-14T12:23:09.265447Z","iopub.status.idle":"2024-08-14T12:23:09.281199Z","shell.execute_reply.started":"2024-08-14T12:23:09.265411Z","shell.execute_reply":"2024-08-14T12:23:09.280305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the model\nnet = CNN20d()\n\n# Print the model summary (optional)\nprint(net)","metadata":{"execution":{"iopub.status.busy":"2024-08-14T12:23:09.282462Z","iopub.execute_input":"2024-08-14T12:23:09.282797Z","iopub.status.idle":"2024-08-14T12:23:09.312902Z","shell.execute_reply.started":"2024-08-14T12:23:09.282771Z","shell.execute_reply":"2024-08-14T12:23:09.311910Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MyDataset(Dataset):\n    \n    def __init__(self, img, label):\n        self.img = torch.Tensor(img.copy())\n        self.label = torch.Tensor(label)\n        self.len = len(img)\n  \n    def __len__(self):\n        return self.len\n\n    def __getitem__(self, idx):\n        return self.img[idx], self.label[idx]","metadata":{"execution":{"iopub.status.busy":"2024-08-14T12:23:09.314178Z","iopub.execute_input":"2024-08-14T12:23:09.314540Z","iopub.status.idle":"2024-08-14T12:23:09.321289Z","shell.execute_reply.started":"2024-08-14T12:23:09.314513Z","shell.execute_reply":"2024-08-14T12:23:09.320295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = MyDataset(np.array(X_train), np.array(y_train))\nval_dataset = MyDataset(np.array(X_val), np.array(y_val))","metadata":{"execution":{"iopub.status.busy":"2024-08-14T12:23:09.322454Z","iopub.execute_input":"2024-08-14T12:23:09.322739Z","iopub.status.idle":"2024-08-14T12:23:09.357887Z","shell.execute_reply.started":"2024-08-14T12:23:09.322709Z","shell.execute_reply":"2024-08-14T12:23:09.356780Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True, pin_memory=True)\nval_dataloader = DataLoader(val_dataset, batch_size=256, shuffle=False, pin_memory=True)","metadata":{"execution":{"iopub.status.busy":"2024-08-14T12:23:09.359210Z","iopub.execute_input":"2024-08-14T12:23:09.359563Z","iopub.status.idle":"2024-08-14T12:23:09.366589Z","shell.execute_reply.started":"2024-08-14T12:23:09.359537Z","shell.execute_reply":"2024-08-14T12:23:09.365558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_loop(dataloader, net, loss_fn, optimizer):\n    \n    running_loss = 0.0\n    current = 0\n    net.train()\n    \n    with tqdm(dataloader) as t:\n        for batch, (X, y) in enumerate(t):\n            X = X.to(device)\n            y = y.to(device)\n            y_pred = net(X)\n            loss = loss_fn(y_pred, y.long())\n            \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            running_loss = (len(X) * loss.item() + running_loss * current) / (len(X) + current)\n            current += len(X)\n            train_acc = (y_pred.argmax(1) == y.long()).sum().item() / len(y_pred)\n            \n            t.set_postfix({'Train Loss':running_loss, 'Train Accuracy:':train_acc})\n    \n    return running_loss","metadata":{"execution":{"iopub.status.busy":"2024-08-14T12:23:09.367970Z","iopub.execute_input":"2024-08-14T12:23:09.368307Z","iopub.status.idle":"2024-08-14T12:23:09.377630Z","shell.execute_reply.started":"2024-08-14T12:23:09.368249Z","shell.execute_reply":"2024-08-14T12:23:09.376797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def val_loop(dataloader, net, loss_fn):\n\n    running_loss = 0.0\n    current = 0\n    net.eval()\n    \n    with torch.no_grad():\n        with tqdm(dataloader) as t:\n            for batch, (X, y) in enumerate(t):\n                X = X.to(device)\n                y = y.to(device)\n                y_pred = net(X)\n                loss = loss_fn(y_pred, y.long())\n\n                running_loss += loss.item()\n                running_loss = (len(X) * running_loss + loss.item() * current) / (len(X) + current)\n                current += len(X)\n                val_acc = (y_pred.argmax(1) == y.long()).sum().item() / len(y_pred)\n                t.set_postfix({'Val Loss':running_loss, 'Val Accuracy:':val_acc})\n            \n    return running_loss","metadata":{"execution":{"iopub.status.busy":"2024-08-14T12:23:09.382086Z","iopub.execute_input":"2024-08-14T12:23:09.382398Z","iopub.status.idle":"2024-08-14T12:23:09.390408Z","shell.execute_reply.started":"2024-08-14T12:23:09.382372Z","shell.execute_reply":"2024-08-14T12:23:09.389492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"net = nn.DataParallel(net)\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(net.parameters(), lr=1e-5)\n\nstart_epoch = 0\nmin_val_loss = 1e9\nlast_min_ind = -1\nearly_stopping_epoch = 50\n\ntb = SummaryWriter()","metadata":{"execution":{"iopub.status.busy":"2024-08-14T12:23:09.391669Z","iopub.execute_input":"2024-08-14T12:23:09.391969Z","iopub.status.idle":"2024-08-14T12:23:10.312022Z","shell.execute_reply.started":"2024-08-14T12:23:09.391945Z","shell.execute_reply":"2024-08-14T12:23:10.311146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create directory to save models\nstart_time = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\nsave_dir = f'run/{start_time}'\nos.makedirs(save_dir, exist_ok=True)\n\ndevice = 'cuda'\n# os.mkdir('../pt'+os.sep+start_time)\nepochs = 50\nfor t in range(start_epoch, epochs):\n    print(f\"Epoch {t}:\")\n    time.sleep(0.2)\n    train_loss = train_loop(train_dataloader, net, loss_fn, optimizer)\n    val_loss = val_loop(val_dataloader, net, loss_fn)\n    tb.add_histogram(\"train_loss\", train_loss, t)\n    torch.save(net, os.path.join(save_dir, 'epoch_{}.pt'.format(t))) \n    if val_loss < min_val_loss:\n        last_min_ind = t\n        min_val_loss = val_loss\n    elif t - last_min_ind >= early_stopping_epoch:\n        break\n\nprint('Done!')\nprint('Best epoch: {}, val_loss: {}'.format(last_min_ind, min_val_loss))","metadata":{"execution":{"iopub.status.busy":"2024-08-14T12:23:10.313118Z","iopub.execute_input":"2024-08-14T12:23:10.313418Z","iopub.status.idle":"2024-08-14T12:23:57.128280Z","shell.execute_reply.started":"2024-08-14T12:23:10.313394Z","shell.execute_reply":"2024-08-14T12:23:57.127286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"net_path = f'run/{start_time}/epoch_{last_min_ind}.pt'\nfinal_net = torch.load(net_path)","metadata":{"execution":{"iopub.status.busy":"2024-08-14T12:23:57.129631Z","iopub.execute_input":"2024-08-14T12:23:57.130008Z","iopub.status.idle":"2024-08-14T12:23:57.143677Z","shell.execute_reply.started":"2024-08-14T12:23:57.129973Z","shell.execute_reply":"2024-08-14T12:23:57.142627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = MyDataset(np.array(X_test), np.array(y_test))\ntest_dataloader = DataLoader(dataset, batch_size=2048, shuffle=False)\n\ndef eval_loop(dataloader, net, loss_fn):\n    \n    running_loss = 0.0\n    total_loss = 0.0\n    current = 0\n    net.eval()\n    target = []\n    predict = []\n    with torch.no_grad():\n        with tqdm(dataloader) as t:\n            for batch, (X, y) in enumerate(t):\n                X = X.to(device)\n                y = y.to(device)\n                y_pred = net(X)\n                target.append(y.detach())\n                predict.append(y_pred.detach())\n                loss = loss_fn(y_pred, y.long())\n                \n                running_loss = (len(X) * loss.item() + running_loss * current) / (len(X) + current)\n                current += len(X)\n                t.set_postfix({'running_loss':running_loss})\n            \n    return total_loss, torch.cat(predict), torch.cat(target)\n\ntest_loss, y_pred, y_target = eval_loop(test_dataloader, final_net, loss_fn)\n\npredict_prob = torch.nn.Softmax(dim=1)(y_pred)\npredicted_classes = predict_prob.argmax(dim=1).cpu().numpy()","metadata":{"execution":{"iopub.status.busy":"2024-08-14T12:23:57.145002Z","iopub.execute_input":"2024-08-14T12:23:57.145307Z","iopub.status.idle":"2024-08-14T12:23:57.290730Z","shell.execute_reply.started":"2024-08-14T12:23:57.145277Z","shell.execute_reply":"2024-08-14T12:23:57.289803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_true = data[(data['Datetime'] >= '2020-12-31') & (data['Datetime'] <= '2023-12-31')]['Close'].reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-08-14T12:23:57.550801Z","iopub.execute_input":"2024-08-14T12:23:57.551800Z","iopub.status.idle":"2024-08-14T12:23:57.559832Z","shell.execute_reply.started":"2024-08-14T12:23:57.551762Z","shell.execute_reply":"2024-08-14T12:23:57.558834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"position1 = np.where(predicted_classes == 0, -1, predicted_classes)\ncurr = position1[0]\nposition2 = [curr]\nfor i in range(1, len(position1)):\n    if curr == 0 or curr == position1[i]:\n        curr = position1[i]\n    else:\n        curr = 0\n    position2.append(curr)","metadata":{"execution":{"iopub.status.busy":"2024-08-14T12:23:57.561148Z","iopub.execute_input":"2024-08-14T12:23:57.561511Z","iopub.status.idle":"2024-08-14T12:23:57.572399Z","shell.execute_reply.started":"2024-08-14T12:23:57.561477Z","shell.execute_reply":"2024-08-14T12:23:57.571389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val = 1\nportf = [1]\nfor i in range(1, len(y_true)):\n    val = val * (1 + position2[i-1] * (y_true[i] - y_true[i-1]) / y_true[i-1])\n    portf.append(val)","metadata":{"execution":{"iopub.status.busy":"2024-08-14T12:23:57.573539Z","iopub.execute_input":"2024-08-14T12:23:57.573832Z","iopub.status.idle":"2024-08-14T12:23:57.600694Z","shell.execute_reply.started":"2024-08-14T12:23:57.573807Z","shell.execute_reply":"2024-08-14T12:23:57.599639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"buy_and_hold = y_true / y_true[0]\ndates = pd.date_range(start='2021-01-01', end='2024-01-01', freq='D')\n\n# Create a new figure\nplt.figure(figsize=(10, 6))\n\n# Plot the data\nplt.plot(dates, buy_and_hold, color='red', label='Buy and Hold')\nplt.plot(dates, portf, color='black', label='CNN')\n\n# Set major formatter and locator for the x-axis\nplt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%m/%Y'))\nplt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=3))\n\n# Add a legend\nplt.legend()\n\n# Automatically format x-axis labels for better readability\nplt.gcf().autofmt_xdate()\n\n# Show the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-14T12:23:57.601950Z","iopub.execute_input":"2024-08-14T12:23:57.602322Z","iopub.status.idle":"2024-08-14T12:23:57.920124Z","shell.execute_reply.started":"2024-08-14T12:23:57.602292Z","shell.execute_reply":"2024-08-14T12:23:57.919157Z"},"trusted":true},"execution_count":null,"outputs":[]}]}